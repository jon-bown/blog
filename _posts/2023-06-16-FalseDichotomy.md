---
layout: post
title: "Existential Risk Debate: A False Dichotomy"
description: ""
date: 2023-06-16
---

There is a growing divide occuring in the Artificial Intelligence (AI) community right now over the long-term risks to humanity that these systems pose. I want to offer my perspective and propose a way forward.

## The Existential Risk Argument

The argument over the existential risk posed by AI is not a recent development but rather a long-standing debate that can be traced back to the dawn of the Industrial Revolution. Perhaps the earliest notable mention of this concept was by Samuel Butler, a Victorian era novelist, in his 1863 essay "Darwin Among the Machines." Butler suggested that machines were a kind of 'mechanical life' evolving and improving at a much faster rate than the biological life from which they had sprung. This idea was of course radical at the time, but has proven to be a recurring theme in the ongoing dialogue about our relationship with machines.

In fact, many of the key figures in the development of computer science and AI have grappled with these ideas. Alan Turing, often considered the father of theoretical computer science and artificial intelligence, was one who paid heed to Butler's argument. Turing famously posed the question "Can machines think?" and pondered the potential of machines taking control, openly attributing Butler's influence on his thoughts in his seminal paper "Computing Machinery and Intelligence" in 1950.

Enter quote here

Fast forward to 2023, and we find a host of other voices contributing to this argument most noteably Geoffrey Hinton and Elon Musk. Hinton, considered the "father of deep learning", has his own perspectives on this issue. Hinton acknowledges the tremendous power of AI but also understands the potential risks that it poses. His focus on creating AI systems that can learn to understand the world in the way that humans do is partly driven by a desire to control those risks, by ensuring that AI develops in a manner that respects human values and goals.

## The 'More Practical Issues' Argument

The most common talking point I hear from this camp is that AGI (Artifical General Intellegence) isn't even liekly to happen soon so why worry about it? 

Andrew Ng's Argument

In today's edition of The Batch, DeepLearning.Ai's newsletter, Andrew seems to pivot a bit and say that while he doesn't see the "existential" risk, he does see a long-term risk to human rights. He uses the resource curse to argue that A.I. could create a scenario where more and more people aren't able to generate enough economic value to justify even a minimum wage which would lead to a consolidation of power and naturally infringements on human rights.

# My Thoughts

I personally don't see how both arguments are all that different. Geoffrey Hinton isn't dissmissive of practical issues at all, even in a recent inverview with PBS he discussed those as part of his postion. Hinton is also NOT arguing to stop AI development. He knows that it can be a source of a tremendous good but is advocating for equal part development and equal part risk mitigation. The perspective sounds completely reasonable to me.

One of the main arguments against the 'existential' camp is that AGI is not going to happen "soon". Since when do we not worry about something becasue its not likely to happen "soon"? Was Alan Turing dismissive of intelligent machines because he didn't think they were possible in his lifetime? It doesn't sound like it, but it also didn't cause him to give up.

This is a very sound argument and I agree that one of the risks of A.I. is that having a more intelligent system around means that the cost of labor would justify hiring less humans. However, I don't see how this argument is different from what the likes of Elon Musk, Hinton are saying. 

Human rights violations throughout history have tended toward mass extinction events. Think about the holocaust, rwanda, the trail of tears, etc. These events were part of a plan to erradicate certain humans from existence. What happens when a more intelligent species than humans is pulling the strings? Ultimately we can't say for certain, but that doesn't mean it has a probability of zero of occuring.

Humans are the dominant species becasue of our intelligence. What happens when we introduce a species that is more intelligent? Evolution says humans take the back seat.

It seems to me that this debate is creating a false dichotomy. I think it is completely reasonable to accept the unknowns with these relatively new technology while also being optomistic about the future of ML/AI. You can understand the risk to humanity that nuclear weapons pose while also understanding why countries have them and develop them. You can also enthusiastically pursue a career in nuclear engineering without being labeled as someone that is furthering the work of destruction.

# A Better Way Forward

Lets move forward with a scientific mindset, not dismissing either perspective based on things we don't know or can't measure. AI is already a disruptive force in our society and we shouldn't assume that it won't continue that trajectory. The risks of A.I. no matter how great or small should be studied and mitigated. Putting things into perspective, A.I. long-term risk is still just as much of a speculation now as it was in 1863. 

# Sources